{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### TENSORBOARD WRITER\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_ROOT_FOLDER = '/Users/brandon/Data/pytorch'\n",
    "\n",
    "# run timestamp\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f'{DATA_ROOT_FOLDER}/runs/{ts}')\n",
    "\n",
    "\n",
    "### DATA\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(root = f'{DATA_ROOT_FOLDER}/data', train = True, download = True, transform = transforms.ToTensor())\n",
    "test_set = torchvision.datasets.FashionMNIST(root = f'{DATA_ROOT_FOLDER}/data', train = False, download = True, transform = transforms.ToTensor())\n",
    "\n",
    "train_val_split = [0.8, 0.2]\n",
    "train_set, val_set = utils.data.random_split(train_set, [round(p * len(train_set)) for p in train_val_split])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# class names\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "\n",
    "class MiniVggBnAfter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiniVggBnAfter, self).__init__()\n",
    "        \n",
    "        # first: CONV => RELU => CONV => RELU => POOL set\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, 3, padding = 1)\n",
    "        self.norm1_1 = nn.BatchNorm2d(32)\n",
    "    \n",
    "        self.conv1_2 = nn.Conv2d(32, 32, 3, padding = 1)\n",
    "        self.norm1_2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # second: CONV => RELU => CONV => RELU => POOL set\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.norm2_1 = nn.BatchNorm2d(64)\n",
    "    \n",
    "        self.conv2_2 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        self.norm2_2 = nn.BatchNorm2d(128)\n",
    "    \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # fully connected (single) to RELU\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 128)\n",
    "        self.normfc_1 = nn.BatchNorm1d(128)    \n",
    "        self.dropoutfc_1 = nn.Dropout1d(0.20)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.norm1_1(F.relu(self.conv1_1(x)))\n",
    "        out = self.norm1_2(F.relu(self.conv1_2(out)))\n",
    "        out = self.pool1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.norm2_1(F.relu(self.conv2_1(out)))\n",
    "        out = self.norm2_2(F.relu(self.conv2_2(out)))\n",
    "        out = self.pool2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # flatten\n",
    "        out = out.view(-1, 128 * 7 * 7)\n",
    "        \n",
    "        out = self.normfc_1(F.relu(self.fc1(out)))\n",
    "        out = self.dropoutfc_1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # softmax classifier\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "model = MiniVggBnAfter()\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(batch_size, 1, 28, 28), verbose=1, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "iter_count = 0\n",
    "for epoch in range(num_epochs):  \n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1:02d}')):\n",
    "        iter_count += 1\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data\n",
    "        # https://github.com/pytorch/pytorch/issues/92311\n",
    "        #train_acc += torch.sum(torch.eq(torch.argmax(outputs, axis=1), labels))\n",
    "        train_acc += torch.sum(torch.eq(torch.max(outputs, axis=1).indices, labels))\n",
    "    \n",
    "        # ...log the running loss\n",
    "        if i and not (i % 100):\n",
    "            writer.add_scalar('Loss/train/100',\n",
    "                            train_loss / i / len(labels),\n",
    "                            len(train_loader.dataset) * epoch + i + 1)\n",
    "\n",
    "            writer.add_scalar('Accuracy/train/100',\n",
    "                            train_acc / i / len(labels),\n",
    "                            len(train_loader.dataset) * epoch + i + 1)\n",
    "    \n",
    "    print('  * train  ' +\n",
    "        f'Loss: {train_loss / len(train_loader.dataset):.4f}, ' +\n",
    "        f'Accuracy: {100 * train_acc / len(train_loader.dataset):.3f}%, ' +\n",
    "        f'LR: {optimizer.param_groups[0][\"lr\"]:.5f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "                \n",
    "            val_loss += loss.data\n",
    "            val_acc += torch.sum(torch.eq(torch.max(outputs, axis=1).indices, labels))\n",
    "            \n",
    "    print('  * val    ' +\n",
    "        f'Loss: {val_loss / len(val_loader.dataset):.4f}, ' +\n",
    "        f'Accuracy: {100 * val_acc / len(val_loader.dataset):.3f}%')\n",
    "\n",
    "    writer.add_scalar('Loss/val',\n",
    "                    val_loss / len(val_loader.dataset),\n",
    "                    epoch+1)\n",
    "\n",
    "    writer.add_scalar('Accuracy/val',\n",
    "                    val_acc / len(val_loader.dataset),\n",
    "                    epoch+1)\n",
    "        \n",
    "    scheduler.step()\n",
    "        \n",
    "\n",
    "test_acc = 0.0\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    with torch.no_grad():\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        test_acc += torch.sum(torch.eq(torch.max(outputs, axis=1).indices, labels))\n",
    "        #test_acc += torch.sum(torch.sum(torch.eq(torch.argmax(outputs, axis=1), labels).long()))\n",
    "        \n",
    "print('  * test    ' +\n",
    "    f'Accuracy: {100 * test_acc / len(test_loader.dataset):.3f}%')\n",
    "\n",
    "# record accuracy\n",
    "writer.add_scalar('test acc',\n",
    "                test_acc / len(test_loader.dataset),\n",
    "                epoch+1)\n",
    "\n",
    "torch.save(model.state_dict(), './fashion_mnist_cnn.pth')\n",
    "#copy = torch.load('./model.pth');\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints with labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)    \n",
    "    output = output.cpu()\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    \n",
    "    images = images.cpu()\n",
    "    \n",
    "    # plot the images in the batch, along with predicted and true labels    \n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TENSORBOARD MODEL\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "images = images.to(device)\n",
    "writer.add_graph(model, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FINAL MODEL\n",
    "# CONFUSION MATRIX\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "confusion_matrix = np.zeros((10, 10))\n",
    "class_count = np.zeros((10))\n",
    "\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    with torch.no_grad():\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        for t, p in zip(labels, preds):\n",
    "            confusion_matrix[t, p] += 1\n",
    "            class_count[t.long()] += 1\n",
    "\n",
    "correct = sum([confusion_matrix[i,i] for i in range(10)])\n",
    "print(f'Overall accuracy: {100*correct/sum(class_count):.2f}%')\n",
    "            \n",
    "plt.figure(figsize=[12,12])\n",
    "df = pd.DataFrame(100 * confusion_matrix / class_count)\n",
    "ax = sn.heatmap(df, vmin=0, vmax=100, cmap='Blues', annot=True, fmt='.2f', annot_kws={\"size\":12}, linewidths=0.5, \n",
    "               xticklabels=classes, yticklabels=classes)\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_ylabel('Prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ee541_work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "15a4d6ee6a255274e34fb02f1a47c210f7fcece87c56c8f35235c009779f4d82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
